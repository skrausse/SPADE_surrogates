configfile: "../configfile.yaml"
import numpy as np
from estimate_number_occurrences import estimate_number_occurrences

# The 5 epochs to analyze
epochs = config['epochs']
# The 4 trial types to analyze
trialtypes = config['trialtypes']
# The sessions to analyze
sessions = config['sessions']
# Absolute (independent of the number of spikes) minimum number of occurrences of a pattern
abs_min_occ = config['abs_min_occ']
# Magnitude of the binsize used
binsize = config['binsize']
# The percentile for the Poisson distribution to fix the minimum number of occ
percentile_poiss = config['percentile_poiss']
# The percentile for the Poisson distribution to fix the minimum number of occ
percentile_rates = config['percentile_rates']
# minimum number of spikes per patterns
abs_min_spikes = config['abs_min_spikes']
# The winlen parameter for the SPADE analysis
winlen = config['winlen']
# Spectrum to use
spectrum = config['spectrum']
# Dithering to use to generate surrogates in seconds
dither = config['dither']
# Number of surrogates to generate
n_surr = config['n_surr']
# Significance level
alpha = config['alpha']
# Multitesting statistical correction
correction = config['correction']
# PSR parameters
psr_param = config['psr_param']
# Unit in which every time of the analysis is expressed
unit = config['unit']
# Firing rate threshold to possibly exclude neurons
firing_rate_threshold = config['firing_rate_threshold']
# Surrogate method to use
surr_method = config['surr_method']
# Data being generated
processes = config['processes']
# The sessions to analyze
sessions = config['sessions']
contexts = [epoch + '_' +tt for epoch in config['epochs'] for tt in config['trialtypes']]

# Create the parameter dictionary
param_dict, excluded_neurons = \
    estimate_number_occurrences(sessions=sessions,
                                         epochs=epochs,
                                         trialtypes=trialtypes,
                                         processes=processes,
                                         binsize=binsize,
                                         abs_min_spikes=abs_min_spikes,
                                         abs_min_occ=abs_min_occ,
                                         correction=correction,
                                         psr_param=psr_param,
                                         alpha=alpha,
                                         n_surr=n_surr,
                                         dither=dither,
                                         spectrum=spectrum,
                                         winlen=winlen,
                                         percentile_poiss=percentile_poiss,
                                         percentile_rates=percentile_rates,
                                         unit=unit,
                                         firing_rate_threshold=firing_rate_threshold,
                                         surr_method=surr_method)

# Rule to collect all the results
rule all:
    input:
         [f'../../results/artificial_data/{surr_method}/{process}/{session}/{context}/{job_id}/results.npy'
         for process in processes for session in sessions for context in contexts for job_id in param_dict[session][process][context]] + [
             f'../../results/artificial_data/{surr_method}/{process}/{session}/{context}/filtered_res.npy'
             for context in contexts for session in sessions for process in processes ]


# Rule to store the new version of the dictionary containing all the parameters
# in the case the configfile changed
rule create_parameter_dict:
    input:
        '../configfile.yaml'
    output:
        'param_dict.npy'
    run:
        np.save('./param_dict.npy', param_dict)

# Rule to run the SPADE analysis on the data (FIM + spectrum computation)
rule analyze_data:
    input:
        parameter_file='param_dict.npy',
        excluded_neurons='excluded_neurons.npy',
        script='spade_analysis.py'
    output:
        '../../results/artificial_data/{surr_method}/{process}/{session}/{context}/{job_id}/results.npy'
    shell:
        "mpirun python spade_analysis.py {wildcards.job_id} {wildcards.context} {wildcards.session} {wildcards.process} {wildcards.surr_method}"

# Function to create output path for filtered results
def select_job_ids_context(wildcards):
    return ['../../results/artificial_data/{surr_method}/{process}/{session}/{context}/{job_id}/results.npy'.format(surr_method=surr_method,
                                                                                                                    job_id=job_id,
                                                                                   process=wildcards.process,
                                                                                   session=wildcards.session,
                                                                                   context=wildcards.context)
            for job_id in param_dict[wildcards.session][wildcards.process][wildcards.context]]

# Rule to apply the PSF and the PSR on the mined patterns
rule filter_results:
    input:
        results=select_job_ids_context,
        script='filter_results.py'
    output:
        '../../results/artificial_data/{surr_method}/{process}/{session}/{context}/filtered_res.npy'
    shell:
        "python filter_results.py {wildcards.context} {wildcards.session} {wildcards.process} {wildcards.surr_method}"

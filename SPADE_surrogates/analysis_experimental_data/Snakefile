configfile: "../configfile.yaml"

import numpy as np
import quantities as pq

import sys 
sys.path.append('../analyse_data_utils')
from estimate_number_occurrences import estimate_number_occurrences

# The 5 epochs to analyze
epochs = config['epochs']
# The 4 trial types to analyze
trialtypes = config['trialtypes']
# The sessions to analyze
sessions = config['sessions']
# Absolute (independent of the number of spikes)
# minimum number of occurrences of a pattern
abs_min_occ = config['abs_min_occ']
# Magnitude of the binsize used
binsize = config['binsize']
# The percentile for the Poisson distribution to fix the minimum number of occ
percentile_poiss = config['percentile_poiss']
# The percentile for the Poisson distribution to fix the minimum number of occ
percentile_rates = config['percentile_rates']
# minimum number of spikes per patterns
abs_min_spikes = config['abs_min_spikes']
# The winlen parameter for the SPADE analysis
winlen = config['winlen']
# Spectrum to use
spectrum = config['spectrum']
# Dithering to use to generate surrogates in seconds
dither = config['dither']
# Number of surrogates to generate
n_surr = config['n_surr']
# Significance level
alpha = config['alpha']
# Multitesting statistical correction
correction = config['correction']
# PSR parameters
psr_param = config['psr_param']
# Unit in which every time of the analysis is expressed
unit = config['unit']
# Firing rate threshold for excluding neurons
firing_rate_threshold = config['firing_rate_threshold']
# Surrogate method to use
surr_method = config['surr_method']

# loading parameters
SNR_thresh = 2.5
synchsize = 2
sep = 2 * winlen * (binsize * pq.s).rescale(unit)

# The sessions to analyze
sessions = config['sessions']
contexts = [epoch + '_' + tt for epoch in config['epochs']
            for tt in config['trialtypes']]

# Create the parameter dictionary
param_dict, excluded_neurons = \
    estimate_number_occurrences(
        sessions=sessions,
        epochs=epochs,
        trialtypes=trialtypes,
        binsize=binsize,
        abs_min_spikes=abs_min_spikes,
        abs_min_occ=abs_min_occ,
        correction=correction,
        psr_param=psr_param,
        alpha=alpha,
        n_surr=n_surr,
        dither=dither,
        spectrum=spectrum,
        winlen=winlen,
        percentile_poiss=percentile_poiss,
        percentile_rates=percentile_rates,
        unit=unit,
        firing_rate_threshold=firing_rate_threshold,
        surr_method=surr_method)


# Rule to collect all the results
rule all:
    input:
        results = expand('../../results/experimental_data/{surr_method}/{session}/{context}/{job_id}/results.npy',
                            surr_method = surr_method,
                            session = sessions,
                            context = contexts,
                            job_id = param_dict[session][context])
        filtered_results = expand('../../results/experimental_data/{surr_method}/{session}/{context}/{job_id}/filtered_res.npy',
                            surr_method = surr_method,
                            session = sessions,
                            context = contexts)

# Rule to store the new version of the dictionary containing all the parameters
# in the case the configfile changed
rule create_parameter_dict:
    input:
        '../configfile.yaml'
    output:
        result = 'param_dict.npy',
        second_result = 'excluded_neurons.npy'
    run:
        np.save('./param_dict.npy', param_dict)
        np.save('./excluded_neurons.npy', excluded_neurons)

# Rule to run the SPADE analysis on the data (FIM + spectrum computation)
rule analyze_data:
    input:
        parameter_file='param_dict.npy',
        excluded_neurons = 'excluded_neurons.npy',
        script='spade_analysis.py'
    output:
        result = '../../results/experimental_data/{surr_method}/{session}/{context}/{job_id}/results.npy'
    shell:
        "python spade_analysis.py {wildcards.job_id} {wildcards.context} {wildcards.session} {wildcards.surr_method}"


# Function to create output path for filtered results
def select_job_ids_context(wildcards):
    return [
        f'../../results/experimental_data/{wildcards.surr_method}'
        f'/{wildcards.session}/{wildcards.context}/{job_id}/results.npy'
        for job_id in param_dict[wildcards.session][wildcards.context]]


# Rule to apply the PSF and the PSR on the mined patterns
rule filter_results:
    input:
        results=select_job_ids_context,
        script='filter_results.py'
    output:
        result = '../../results/experimental_data/{surr_method}/{session}/{context}/filtered_res.npy'
    shell:
        "python filter_results.py {wildcards.context} {wildcards.session} {wildcards.surr_method}"
